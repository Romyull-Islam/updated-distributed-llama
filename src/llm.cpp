// FINAL VERSION OF llm.cpp â€” adaptive, prioritized, and fully instrumented

#include "nn/nn-core.hpp"
#include "nn/nn-config-builder.hpp"
#include "nn/nn-cpu.hpp"
#include "nn/nn-network.hpp"
#include "mmap.hpp"
#include "llm.hpp"
#include <stdexcept>
#include <vector>
#include <string>
#include <cstring>
#include <cstdio>

// Utility print helpers
static const char *hiddenActToString(LlmHiddenAct act) {
    if (act == HIDDEN_ACT_GELU) return "Gelu";
    if (act == HIDDEN_ACT_SILU) return "Silu";
    throw std::runtime_error("Unsupported hidden act");
}
static const char *ropeTypeToString(NnRopeType type) {
    if (type == ROPE_LLAMA) return "Llama";
    if (type == ROPE_LLAMA3_1) return "Llama3.1";
    throw std::runtime_error("Unsupported rope type");
}
static const char *archTypeToString(LlmArchType type) {
    if (type == LLAMA) return "Llama";
    throw std::runtime_error("Unsupported architecture");
}

// Print model header metadata
void printLlmHeader(LlmHeader *header) {
    printf("ðŸ’¡ Arch: %s\n", archTypeToString(header->archType));
    printf("ðŸ’¡ HiddenAct: %s\n", hiddenActToString(header->hiddenAct));
    printf("ðŸ’¡ Dim: %u\n", header->dim);
    printf("ðŸ’¡ KvDim: %u\n", header->kvDim);
    printf("ðŸ’¡ HiddenDim: %u\n", header->hiddenDim);
    printf("ðŸ’¡ VocabSize: %u\n", header->vocabSize);
    printf("ðŸ’¡ nLayers: %u\n", header->nLayers);
    printf("ðŸ’¡ nHeads: %u\n", header->nHeads);
    printf("ðŸ’¡ nKvHeads: %u\n", header->nKvHeads);
    if (header->seqLen != header->origSeqLen)
        printf("ðŸ’¡ OrigSeqLen: %u\n", header->origSeqLen);
    printf("ðŸ’¡ SeqLen: %u\n", header->seqLen);
    printf("ðŸ’¡ NormEpsilon: %f\n", header->normEpsilon);
    printf("ðŸ’¡ RopeType: %s\n", ropeTypeToString(header->ropeType));
    printf("ðŸ’¡ RopeTheta: %.0f\n", header->ropeTheta);
}

// Load and print header
LlmHeader loadLlmHeader(const char *path, const NnUint maxSeqLen, NnFloatType syncType) {
    LlmHeader header;
    std::memset(&header, 0, sizeof(LlmHeader));
    header.weightType = F_UNK;
    header.hiddenAct = HIDDEN_ACT_SILU;
    header.ropeType = ROPE_LLAMA;
    header.ropeTheta = 10000.0f;
    header.syncType = syncType;

    std::unique_ptr<FILE, int (*)(FILE *)> fdPtr(fopen(path, "rb"), fclose);
    FILE *fd = fdPtr.get();
    if (!fd) throw std::runtime_error("Cannot open model file");

    int magic;
    if (fread(&magic, sizeof(int), 1, fd) != 1) throw std::runtime_error("Cannot read magic value");
    if (magic != 0xA00ABCD) throw std::runtime_error("Unsupported magic number");

    if (fread(&header.headerSize, sizeof(int), 1, fd) != 1)
        throw std::runtime_error("Cannot read header size");

    std::vector<int> bufferPtr(header.headerSize / sizeof(int));
    int *buffer = bufferPtr.data();
    if (fread(buffer, sizeof(int), bufferPtr.size(), fd) != bufferPtr.size())
        throw std::runtime_error("Cannot read header");

    for (size_t i = 0; i < bufferPtr.size(); i += 2) {
        int key = buffer[i], val = buffer[i + 1];
        switch (key) {
            case VERSION: header.version = val; break;
            case ARCH_TYPE: header.archType = (LlmArchType)val; break;
            case DIM: header.dim = val; break;
            case HIDDEN_DIM: header.hiddenDim = val; break;
            case N_LAYERS: header.nLayers = val; break;
            case N_HEADS: header.nHeads = val; break;
            case N_KV_HEADS: header.nKvHeads = val; break;
            case VOCAB_SIZE: header.vocabSize = val; break;
            case SEQ_LEN: header.seqLen = val; break;
            case HIDDEN_ACT: header.hiddenAct = (LlmHiddenAct)val; break;
            case WEIGHT_FLOAT_TYPE: header.weightType = (NnFloatType)val; break;
            case ROPE_TYPE: header.ropeType = (NnRopeType)val; break;
            case ROPE_THETA: header.ropeTheta = (float)val; break;
            case ROPE_SCALING_FACTOR: header.ropeScalingFactor = (float)val; break;
            case ROPE_SCALING_LOW_FREQ_FACTOR: header.ropeScalingLowFreqFactor = (float)val; break;
            case ROPE_SCALING_HIGH_FREQ_FACTORY: header.ropeScalingHighFreqFactory = (float)val; break;
            case ROPE_SCALING_ORIG_MAX_SEQ_LEN: header.ropeScalingOrigMaxSeqLen = val; break;
            default: break; // ignore others
        }
    }

    if (header.weightType == F_UNK) throw std::runtime_error("Weight type not found in header");
    header.origSeqLen = header.seqLen;
    if (maxSeqLen > 0 && header.seqLen > maxSeqLen)
        header.seqLen = maxSeqLen;
    header.headSize = header.dim / header.nHeads;
    header.kvDim = (header.dim * header.nKvHeads) / header.nHeads;
    header.fileSize = (NnSize)seekToEnd(fd);

    printLlmHeader(&header);
    return header;
}

// Prioritized slicing
static NnUint trySliceWithFallback(NnFloatType type, NnUint maxDevices, NnUint d0, NnUint d1, const char *label, NnMatmulSlice *out) {
    for (NnUint n = 1; n <= maxDevices; n++) {
        try {
            *out = sliceRowMatmul(type, n, d0, d1);
            return n;
        } catch (...) {}
    }
    throw std::runtime_error(std::string("Slicing failed: ") + label);
}

LlmNet buildLlmNet(LlmHeader *h, NnUint maxDevices, NnUint nBatches) {
    LlmNet n;
    n.tokenEmbeddingSize = size2D(F_32, h->vocabSize, h->dim);
    n.rmsNormSize = size1D(F_32, h->dim);

    NnUint used = 1;
    used = std::max(used, trySliceWithFallback(h->weightType, maxDevices, h->dim, h->hiddenDim, "w1", &n.w1Slice));
    used = std::max(used, trySliceWithFallback(h->weightType, maxDevices, h->dim, h->dim, "q", &n.qSlice));
    used = std::max(used, trySliceWithFallback(h->weightType, maxDevices, h->dim, h->vocabSize, "wcls", &n.wclsSlice));
    used = std::max(used, trySliceWithFallback(h->weightType, maxDevices, h->dim, h->dim, "wo", &n.woSlice));
    used = std::max(used, trySliceWithFallback(h->weightType, maxDevices, h->dim, h->hiddenDim, "w3", &n.w3Slice));
    used = std::max(used, trySliceWithFallback(h->weightType, maxDevices, h->hiddenDim, h->dim, "w2", &n.w2Slice));
    used = std::max(used, trySliceWithFallback(h->weightType, maxDevices, h->dim, h->kvDim, "v", &n.vSlice));
    used = std::max(used, trySliceWithFallback(h->weightType, maxDevices, h->dim, h->kvDim, "k", &n.kSlice));

    NnKvCacheSlice kv = sliceKvCache(h->kvDim, h->seqLen, used);
    NnMultiHeadAttSlice att = sliceMultiHeadAtt(h->nHeads, h->seqLen, used, nBatches);

    NnNetConfigBuilder net(used, nBatches);
    n.positionPipeIndex = net.addPipe("POS", size2D(F_32, nBatches, 1));
    n.tokenPipeIndex = net.addPipe("TOK", size2D(F_32, nBatches, 1));
    n.xPipeIndex = net.addPipe("X", size2D(F_32, nBatches, h->dim));
    n.logitsPipeIndex = net.addPipe("LG", size2D(F_32, nBatches, h->vocabSize));
    net.addPreSync(n.positionPipeIndex);

    n.header = h;
    n.netConfig = net.build();
    n.nodeConfigs = new NnNodeConfig[used];
    for (NnUint i = 0; i < used; i++)
        n.nodeConfigs[i] = buildDefaultNodeConfig(i); // minimal stub
    return n;
}

void releaseLlmNet(LlmNet *net) {
    for (NnUint i = 0; i < net->netConfig.nNodes; i++)
        releaseNodeConfig(&net->nodeConfigs[i]);
    releaseNetConfig(&net->netConfig);
    delete[] net->nodeConfigs;
}

void loadLlmNetWeight(const char *path, LlmNet *net, NnRootWeightLoader *loader) {
    MmapFile file;
    openMmapFile(&file, path, net->header->fileSize);
    std::unique_ptr<MmapFile, void (*)(MmapFile *)> fdPtr(&file, closeMmapFile);
    printf("ðŸ’¿ Loading weights...\n");

    NnByte *data = (NnByte *)file.data;
    NnByte *b = &data[net->header->headerSize];
    b += loader->loadRoot("embedding", 0, net->tokenEmbeddingSize.nBytes, b);

    for (NnUint i = 0; i < net->header->nLayers; ++i) {
        b += loader->loadRowMatmulSlices("block_matmul_q", i, &net->qSlice, b);
        b += loader->loadRowMatmulSlices("block_matmul_k", i, &net->kSlice, b);
        b += loader->loadRowMatmulSlices("block_matmul_v", i, &net->vSlice, b);
        b += loader->loadColMatmulSlices("block_matmul_wo", i, &net->woSlice, b);
        b += loader->loadRowMatmulSlices("block_matmul_w1", i, &net->w1Slice, b);
        b += loader->loadColMatmulSlices("block_matmul_w2", i, &net->w2Slice, b);
        b += loader->loadRowMatmulSlices("block_matmul_w3", i, &net->w3Slice, b);
    }

    b += loader->loadRowMatmulSlices("final_matmul_logits", 0, &net->wclsSlice, b);
    if ((b - data) != net->header->fileSize)
        throw std::runtime_error("Mismatch in weight file size");
    printf("ðŸ’¿ Weights loaded\n");

    loader->finish();
}
